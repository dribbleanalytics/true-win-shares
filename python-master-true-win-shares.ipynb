{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for models\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load game logs data\n",
    "\n",
    "df = pd.read_csv('full_game_logs.csv')\n",
    "df_pred = pd.read_csv('full_game_logs_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify model features and outputs\n",
    "\n",
    "features = ['home_away', 'mp', '3p', 'ft', 'trb', 'ast', 'stl', 'blk', 'tov', 'pts']\n",
    "output = ['win_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grid search function to return best parameters for each model\n",
    "\n",
    "cv = StratifiedKFold(n_splits = 3, random_state = 0)\n",
    "\n",
    "def grid_search(model, grid):\n",
    "    clf = GridSearchCV(model, grid, cv = cv, n_jobs = -1, verbose = 2, iid = False, scoring = 'accuracy')\n",
    "    scores(clf)\n",
    "    \n",
    "    print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to evaluate model performance\n",
    "\n",
    "def scores(model):\n",
    "    \n",
    "    model.fit(xtrain, ytrain.values.ravel())\n",
    "    y_pred = model.predict(xtest)\n",
    "    \n",
    "    print(\"Accuracy score: %.3f\" % metrics.accuracy_score(ytest, y_pred))\n",
    "    print(\"Recall: %.3f\" % metrics.recall_score(ytest, y_pred))\n",
    "    print(\"Precision: %.3f\" % metrics.precision_score(ytest, y_pred))\n",
    "    print(\"F1: %.3f\" % metrics.f1_score(ytest, y_pred))\n",
    "    \n",
    "    proba = model.predict_proba(xtest)\n",
    "    print(\"Log loss: %.3f\" % metrics.log_loss(ytest, proba))\n",
    "\n",
    "    pos_prob = proba[:, 1]\n",
    "    print(\"Area under ROC curve: %.3f\" % metrics.roc_auc_score(ytest, pos_prob))\n",
    "    \n",
    "    cv = cross_val_score(model, xtest, ytest.values.ravel(), cv = 3, scoring = 'accuracy')\n",
    "    print(\"Accuracy (cross validation score): %0.3f (+/- %0.3f)\" % (cv.mean(), cv.std() * 2))\n",
    "    \n",
    "    cv = cross_val_score(model, xtest, ytest.values.ravel(), cv = 3, scoring = 'recall')\n",
    "    print(\"Recall (cross validation score): %0.3f (+/- %0.3f)\" % (cv.mean(), cv.std() * 2))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create stat line models from 2014-15 to 2017-18 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run grid search cv on randomly selected subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select 10% of the data set for the purpose of running grid search on it\n",
    "\n",
    "df_sample = df.sample(n = int(len(df) / 10))\n",
    "\n",
    "train, test = train_test_split(df_sample, test_size = 0.25, random_state = 0)\n",
    "\n",
    "xtrain = train[features]\n",
    "ytrain = train[output]\n",
    "\n",
    "xtest = test[features]\n",
    "ytest = test[output]\n",
    "\n",
    "print(\"Training set size: %.0f\" % len(xtrain))\n",
    "print(\"Testing set size: %.0f\" % len(xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = LogisticRegression(solver = 'liblinear')\n",
    "\n",
    "y_log = scores(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [int(x) for x in np.linspace(start = 1, stop = 50, num = 20)]\n",
    "penalty = ['l1', 'l2']\n",
    "solver = ['liblinear']\n",
    "\n",
    "grid = {'C': C,\n",
    "        'penalty': penalty,\n",
    "        'solver': solver}\n",
    "\n",
    "grid_search(log, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "y_lda = scores(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = ['svd', 'lsqr', 'eigen']\n",
    "\n",
    "grid = {'solver': solver}\n",
    "\n",
    "grid_search(lda, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 100, random_state = 0)\n",
    "\n",
    "y_rf = scores(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = [int(x) for x in np.linspace(start = 10, stop = 100, num = 10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "n_estimators = [int(x) for x in np.linspace(start = 25, stop = 250, num = 10)]\n",
    "random_state = [0]\n",
    "\n",
    "grid = {'max_depth': max_depth,\n",
    "        'max_features': max_features,\n",
    "        'n_estimators': n_estimators,\n",
    "        'random_state': random_state}\n",
    "\n",
    "grid_search(rf, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(random_state = 0)\n",
    "\n",
    "y_gbc = scores(gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ['deviance']\n",
    "max_depth = [int(x) for x in np.linspace(start = 10, stop = 100, num = 10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "n_estimators = [int(x) for x in np.linspace(start = 25, stop = 250, num = 10)]\n",
    "random_state = [0]\n",
    "\n",
    "grid = {'loss': loss,\n",
    "        'max_depth': max_depth,\n",
    "        'max_features': max_features,\n",
    "        'n_estimators': n_estimators,\n",
    "        'random_state': random_state}\n",
    "\n",
    "grid_search(gbc, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = MLPClassifier(max_iter=1000, random_state = 0)\n",
    "\n",
    "y_dnn = scores(dnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes = [(50, 50, 50), (100, 50, 25), (100, 100, 100)]\n",
    "activation = ['tanh', 'relu']\n",
    "solver = ['sgd', 'adam']\n",
    "alpha = [.0001, .001, .01, .05]\n",
    "learning_rate = ['constant', 'adaptive']\n",
    "max_iter = [1000]\n",
    "random_state = [0]\n",
    "\n",
    "grid = {'hidden_layer_sizes': hidden_layer_sizes,\n",
    "        'activation': activation,\n",
    "        'solver': solver,\n",
    "        'alpha': alpha,\n",
    "        'learning_rate': learning_rate,\n",
    "        'max_iter' : max_iter,\n",
    "        'random_state': random_state}\n",
    "\n",
    "grid_search(dnn, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create models on full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, use hyperparmeters from grid search on full data set\n",
    "\n",
    "train, test = train_test_split(df, test_size = 0.25, random_state = 0)\n",
    "\n",
    "xtrain = train[features]\n",
    "ytrain = train[output]\n",
    "\n",
    "xtest = test[features]\n",
    "ytest = test[output]\n",
    "\n",
    "print(\"Training set size: %.0f\" % len(xtrain))\n",
    "print(\"Testing set size: %.0f\" % len(xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find ytrain class distribution\n",
    "\n",
    "sum(ytrain['win_loss']) / len(ytrain['win_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = LogisticRegression(solver = 'liblinear')\n",
    "\n",
    "y_log = scores(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(solver = 'svd')\n",
    "\n",
    "y_lda = scores(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth = 10, max_features = 'auto', n_estimators = 75, random_state = 0)\n",
    "\n",
    "y_rf = scores(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(random_state = 0)\n",
    "\n",
    "y_gbc = scores(gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = MLPClassifier(activation = 'relu', alpha = 0.01, hidden_layer_sizes = (100, 50, 25), learning_rate = 'adaptive',\n",
    "                    max_iter=1000, random_state = 0, solver = 'sgd')\n",
    "\n",
    "y_dnn = scores(dnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy= \"stratified\", random_state = 0)\n",
    "\n",
    "y_dummy = scores(dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create confusion matrices for each model\n",
    "\n",
    "def confusion_matrix(y_pred, model_name):\n",
    "    cm = metrics.confusion_matrix(ytest, y_pred)\n",
    "\n",
    "    plt.style.use(\"fivethirtyeight\")\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    sns.heatmap(cm, annot=True, ax = ax, linewidth = 2, fmt='g')\n",
    "\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "\n",
    "    fig.suptitle(\"%s Confusion Matrix\" % model_name.upper(), weight = 'bold', size = 18, x = .45)\n",
    "    \n",
    "    fig.text(x = -0.02, y = -0.08,\n",
    "        s = '__________________________________________________________',\n",
    "        fontsize = 14, color = 'grey', horizontalalignment='left')\n",
    "\n",
    "    fig.text(x = -0.02, y = -.14,\n",
    "        s = 'https://dribbleanalytics.blog                     ',\n",
    "        fontsize = 14, fontname = 'Rockwell', color = 'grey', horizontalalignment='left')\n",
    "\n",
    "    fig.savefig('%s_cm.png' % model_name, dpi = 400, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_log, 'log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_lda, 'lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_rf, 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_gbc, 'gbc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_dnn, 'dnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict wins given stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create predictions from each model\n",
    "\n",
    "def make_pred(model_list, df_pred):\n",
    "    prob_list = []\n",
    "    \n",
    "    for i in model_list:\n",
    "        proba = i.predict_proba(df_pred)\n",
    "        pos_prob = proba[:, 1]\n",
    "        prob_list.append(pos_prob)\n",
    "        \n",
    "    return prob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset 2018-19 data to those who played at least 41 games, then calculate their true win shares\n",
    "\n",
    "game_count = df_pred.groupby(['player'])['g'].count()\n",
    "game_index = game_count[game_count > 41].index\n",
    "\n",
    "df_pred = df_pred[df_pred['player'].isin(game_index)].reset_index(drop = True)\n",
    "\n",
    "prob_list = make_pred([log, lda, rf, gbc, dnn], df_pred[features])\n",
    "\n",
    "pred_vals = pd.DataFrame(data = np.transpose(prob_list), columns = ['log', 'lda', 'rf', 'gbc', 'dnn'])\n",
    "\n",
    "pred_vals['avg'] = (pred_vals['log'] + pred_vals['lda'] + pred_vals['rf'] + pred_vals['gbc'] + pred_vals['dnn']) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games_pred = df_pred.join(pred_vals)\n",
    "df_games_pred['cumulative_tws'] = df_games_pred.groupby(['player'])['avg'].transform(pd.Series.cumsum)\n",
    "\n",
    "df_games_pred.to_csv('game-log-pred.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare true win shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_csv('game-log-pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate true win shares by player\n",
    "\n",
    "tws_sum = df_results.groupby(['player'])[['log', 'lda', 'rf', 'gbc', 'dnn', 'avg']].sum().sort_values(\n",
    "    ascending = False, by = 'avg')\n",
    "\n",
    "tws_sum = tws_sum.reset_index()\n",
    "\n",
    "tws_sum.to_csv('cumulative-tws.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tws_avg = df_results.groupby(['player'])[['log', 'lda', 'rf', 'gbc', 'dnn', 'avg']].mean().sort_values(\n",
    "    ascending = False, by = 'avg')\n",
    "\n",
    "tws_avg = tws_avg.reset_index()\n",
    "\n",
    "tws_avg.to_csv('average-tws.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
